{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "changing-cylinder",
   "metadata": {
    "papermill": {
     "duration": 0.021577,
     "end_time": "2021-05-25T01:24:39.055187",
     "exception": false,
     "start_time": "2021-05-25T01:24:39.033610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Amazon SageMaker script mode using Scikit Learn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ca9be18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ The latest SageMaker Distribution image version known to work with this notebook is <code>3.3.0</code>. If you encounter problems with other versions, please downgrade to version <code>3.3.0</code>. <b>To do so, you must stop your JupyterApp, downgrade the SageMaker Distribution image to <code>3.3.0</code> and restart the JupyterLabApp for the changes to take effect</b>.</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74522624",
   "metadata": {
    "papermill": {
     "duration": 0.021577,
     "end_time": "2021-05-25T01:24:39.055187",
     "exception": false,
     "start_time": "2021-05-25T01:24:39.033610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This lab demonstrates how to deploy a single machine learning model using Amazon SageMaker with `Scikit-Learn` in script mode. Script mode allows you to use your own training scripts with greater flexibility while leveraging SageMaker's managed training and deployment infrastructure.\n",
    "\n",
    "With SageMaker script mode, you can:\n",
    "\n",
    "- Write your own custom training code using familiar frameworks\n",
    "- Maintain complete control over your algorithms and models\n",
    "- Easily transition existing Scikit-Learn code to run on SageMaker\n",
    "- Take advantage of SageMaker's optimized infrastructure for training and hosting\n",
    "\n",
    "In this lab, we'll build and deploy a Scikit-Learn model that predicts housing prices based on various features. This straightforward use case provides an excellent foundation to understand how SageMaker script mode works with the Scikit-Learn framework.\n",
    "\n",
    "SageMaker's Scikit-Learn container includes the framework and dependencies needed to train and serve models, while script mode gives you the flexibility to customize your training process. This approach combines the convenience of managed infrastructure with the control data scientists need over their modeling workflow.\n",
    "\n",
    "The techniques demonstrated in this notebook can be applied to other machine learning problems where you want to leverage your own custom Scikit-Learn code while taking advantage of SageMaker's scalable training and deployment capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-subject",
   "metadata": {
    "papermill": {
     "duration": 0.020745,
     "end_time": "2021-05-25T01:24:39.138697",
     "exception": false,
     "start_time": "2021-05-25T01:24:39.117952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate synthetic data for housing model\n",
    "\n",
    "The code below contains helper functions to generate synthetic data representing the features of a house.\n",
    "\n",
    "The first entry in the array is the randomly generated price of a house. The remaining entries are the features (i.e. number of bedroom, square feet, number of bathrooms, etc.).\n",
    "\n",
    "These functions will be used to generate synthetic data for training, validation, and testing. It will also allow us to submit synthetic payloads for inference to test our endpoint from a model trained in script mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa9b9a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-cooking",
   "metadata": {
    "papermill": {
     "duration": 0.227019,
     "end_time": "2021-05-25T01:24:39.386441",
     "exception": false,
     "start_time": "2021-05-25T01:24:39.159422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-remains",
   "metadata": {
    "papermill": {
     "duration": 0.026273,
     "end_time": "2021-05-25T01:24:39.433607",
     "exception": false,
     "start_time": "2021-05-25T01:24:39.407334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_HOUSES = 1000\n",
    "LOCATION = \"NewYork_NY\"\n",
    "MAX_YEAR = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-durham",
   "metadata": {
    "papermill": {
     "duration": 0.027218,
     "end_time": "2021-05-25T01:24:39.481669",
     "exception": false,
     "start_time": "2021-05-25T01:24:39.454451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_price(house):\n",
    "    _base_price = int(house[\"SQUARE_FEET\"] * 150)\n",
    "    _price = int(\n",
    "        _base_price\n",
    "        + (10000 * house[\"NUM_BEDROOMS\"])\n",
    "        + (15000 * house[\"NUM_BATHROOMS\"])\n",
    "        + (15000 * house[\"LOT_ACRES\"])\n",
    "        + (15000 * house[\"GARAGE_SPACES\"])\n",
    "        - (5000 * (MAX_YEAR - house[\"YEAR_BUILT\"]))\n",
    "    )\n",
    "    return _price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-refrigerator",
   "metadata": {
    "papermill": {
     "duration": 0.028183,
     "end_time": "2021-05-25T01:24:39.530684",
     "exception": false,
     "start_time": "2021-05-25T01:24:39.502501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_random_house():\n",
    "    _house = {\n",
    "        \"SQUARE_FEET\": int(np.random.normal(3000, 750)),\n",
    "        \"NUM_BEDROOMS\": np.random.randint(2, 7),\n",
    "        \"NUM_BATHROOMS\": np.random.randint(2, 7) / 2,\n",
    "        \"LOT_ACRES\": round(np.random.normal(1.0, 0.25), 2),\n",
    "        \"GARAGE_SPACES\": np.random.randint(0, 4),\n",
    "        \"YEAR_BUILT\": min(MAX_YEAR, int(np.random.normal(1995, 10))),\n",
    "    }\n",
    "    _price = gen_price(_house)\n",
    "    return [\n",
    "        _price,\n",
    "        _house[\"YEAR_BUILT\"],\n",
    "        _house[\"SQUARE_FEET\"],\n",
    "        _house[\"NUM_BEDROOMS\"],\n",
    "        _house[\"NUM_BATHROOMS\"],\n",
    "        _house[\"LOT_ACRES\"],\n",
    "        _house[\"GARAGE_SPACES\"],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-gauge",
   "metadata": {
    "papermill": {
     "duration": 0.026726,
     "end_time": "2021-05-25T01:24:39.578209",
     "exception": false,
     "start_time": "2021-05-25T01:24:39.551483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_houses(num_houses):\n",
    "    _house_list = []\n",
    "    for i in range(num_houses):\n",
    "        _house_list.append(gen_random_house())\n",
    "    _df = pd.DataFrame(\n",
    "        _house_list,\n",
    "        columns=[\n",
    "            \"PRICE\",\n",
    "            \"YEAR_BUILT\",\n",
    "            \"SQUARE_FEET\",\n",
    "            \"NUM_BEDROOMS\",\n",
    "            \"NUM_BATHROOMS\",\n",
    "            \"LOT_ACRES\",\n",
    "            \"GARAGE_SPACES\",\n",
    "        ],\n",
    "    )\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-arlington",
   "metadata": {
    "papermill": {
     "duration": 0.020962,
     "end_time": "2021-05-25T01:24:39.619966",
     "exception": false,
     "start_time": "2021-05-25T01:24:39.599004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train a house value prediction model\n",
    "\n",
    "In the follow section, we are setting up the code to train a house price prediction model.\n",
    "\n",
    "As such, we will launch a training job using the AWS Managed container for Scikit Learn via the Sagemaker SDK using the `SKLearn` estimator class.\n",
    "\n",
    "In this notebook, we will be using the AWS Managed Scikit Learn image for both training and inference - this image provides native support for launching endpoints backed by SKLearn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-software",
   "metadata": {
    "papermill": {
     "duration": 1.014268,
     "end_time": "2021-05-25T01:24:40.654888",
     "exception": false,
     "start_time": "2021-05-25T01:24:39.640620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "BUCKET = sagemaker_session.default_bucket()\n",
    "TRAINING_FILE = \"training.py\"\n",
    "INFERENCE_FILE = \"inference.py\"\n",
    "SOURCE_DIR = \"source_dir\"\n",
    "\n",
    "DATA_PREFIX = \"DEMO_SCRIPT_MODE_SCIKIT_V1\"\n",
    "MODEL_ARTIFACTS = \"model_artifacts\"\n",
    "\n",
    "TRAIN_INSTANCE_TYPE = \"ml.m5.xlarge\"\n",
    "ENDPOINT_INSTANCE_TYPE = \"ml.m5.xlarge\"\n",
    "\n",
    "CUR = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "ENDPOINT_NAME = \"script-mode-sklearn-housing-V1\" + \"-\" + CUR\n",
    "\n",
    "MODEL_NAME = ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-minutes",
   "metadata": {
    "papermill": {
     "duration": 0.020789,
     "end_time": "2021-05-25T01:24:40.696843",
     "exception": false,
     "start_time": "2021-05-25T01:24:40.676054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Split a given dataset into train, validation, and test\n",
    "\n",
    "The code below will generate 3 sets of data. 1 set to train, 1 set for validation and 1 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-hours",
   "metadata": {
    "papermill": {
     "duration": 0.203792,
     "end_time": "2021-05-25T01:24:40.921296",
     "exception": false,
     "start_time": "2021-05-25T01:24:40.717504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 7\n",
    "SPLIT_RATIOS = [0.6, 0.3, 0.1]\n",
    "\n",
    "\n",
    "def split_data(df):\n",
    "    # split data into train and test sets\n",
    "    seed = SEED\n",
    "    val_size = SPLIT_RATIOS[1]\n",
    "    test_size = SPLIT_RATIOS[2]\n",
    "\n",
    "    num_samples = df.shape[0]\n",
    "    X1 = df.values[:num_samples, 1:]  # keep only the features, skip the target, all rows\n",
    "    Y1 = df.values[:num_samples, :1]  # keep only the target, all rows\n",
    "\n",
    "    # Use split ratios to divide up into train/val/test\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X1, Y1, test_size=(test_size + val_size), random_state=seed\n",
    "    )\n",
    "    # Of the remaining non-training samples, give proper ratio to validation and to test\n",
    "    X_test, X_test, y_test, y_test = train_test_split(\n",
    "        X_val, y_val, test_size=(test_size / (test_size + val_size)), random_state=seed\n",
    "    )\n",
    "    # reassemble the datasets with target in first column and features after that\n",
    "    _train = np.concatenate([y_train, X_train], axis=1)\n",
    "    _val = np.concatenate([y_val, X_val], axis=1)\n",
    "    _test = np.concatenate([y_test, X_test], axis=1)\n",
    "\n",
    "    return _train, _val, _test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-salad",
   "metadata": {
    "papermill": {
     "duration": 0.021091,
     "end_time": "2021-05-25T01:24:40.963458",
     "exception": false,
     "start_time": "2021-05-25T01:24:40.942367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Prepare training and inference scripts\n",
    "\n",
    "By using the Scikit Learn estimator via the Sagemaker SDK, we can host and train models on Amazon Sagemaker.\n",
    "\n",
    "For training, we do the following:\n",
    "\n",
    "1. Prepare a training script - this script will execute the training logic within a SageMaker managed Scikit Learn container.\n",
    "\n",
    "\n",
    "2. Create a `sagemaker.sklearn.estimator.SKLearn` estimator\n",
    "\n",
    "\n",
    "3. Call the estimators `.fit()` method.\n",
    "\n",
    "For more information on using scikit learn with the Sagemaker SDK, see the docs [here.](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html)\n",
    "\n",
    "Below, we will create the training script called `training.py` that will be located at the root of a dicrectory called `source_dir`.\n",
    "\n",
    "In this example, we will be training a [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) model that will later be used for inference in predicting house prices.\n",
    "\n",
    "**NOTE:** You would modify the script below to implement your own training logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-lewis",
   "metadata": {
    "papermill": {
     "duration": 0.177685,
     "end_time": "2021-05-25T01:24:41.162133",
     "exception": false,
     "start_time": "2021-05-25T01:24:40.984448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir $SOURCE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-prediction",
   "metadata": {
    "papermill": {
     "duration": 0.028608,
     "end_time": "2021-05-25T01:24:41.211968",
     "exception": false,
     "start_time": "2021-05-25T01:24:41.183360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $SOURCE_DIR/$TRAINING_FILE\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    # to simplify the demo we don't use all sklearn RandomForest hyperparameters\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=10)\n",
    "    parser.add_argument(\"--min-samples-leaf\", type=int, default=3)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--validation\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\"))\n",
    "    parser.add_argument(\"--model-name\", type=str)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"reading data\")\n",
    "    print(\"model_name: {}\".format(args.model_name))\n",
    "\n",
    "    train_file = os.path.join(args.train, args.model_name + \"_train.csv\")\n",
    "    train_df = pd.read_csv(train_file)  # read in the training data\n",
    "\n",
    "    val_file = os.path.join(args.validation, args.model_name + \"_val.csv\")\n",
    "    test_df = pd.read_csv(os.path.join(val_file))  # read in the test data\n",
    "\n",
    "    # Matrix representation of the data\n",
    "    print(\"building training and testing datasets\")\n",
    "    X_train = train_df[train_df.columns[1 : train_df.shape[1]]]\n",
    "    X_test = test_df[test_df.columns[1 : test_df.shape[1]]]\n",
    "    y_train = train_df[train_df.columns[0]]\n",
    "    y_test = test_df[test_df.columns[0]]\n",
    "\n",
    "    # fitting the model\n",
    "    print(\"training model\")\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=args.n_estimators, min_samples_leaf=args.min_samples_leaf, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # print abs error\n",
    "    print(\"validating model\")\n",
    "    abs_err = np.abs(model.predict(X_test) - y_test)\n",
    "\n",
    "    # print couple perf metrics\n",
    "    for q in [10, 50, 90]:\n",
    "        print(\"AE-at-\" + str(q) + \"th-percentile: \" + str(np.percentile(a=abs_err, q=q)))\n",
    "\n",
    "    # persist model\n",
    "    path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, path)\n",
    "    print(\"model persisted at \" + path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-rwanda",
   "metadata": {
    "papermill": {
     "duration": 0.020987,
     "end_time": "2021-05-25T01:24:41.254385",
     "exception": false,
     "start_time": "2021-05-25T01:24:41.233398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When using multi-model endpoints with the Sagemaker managed Scikit Learn container, we need to provide an entry point script for\n",
    "inference that will **at least** load the saved model.\n",
    "\n",
    "We will now create this script and call it `inference.py` and store it at the root of a directory called `source_dir`. This is the same directory which contains our `training.py` script.\n",
    "\n",
    "**Note:** You could place the below `model_fn` function within the `training.py` script (above the main guard) if you prefer to have a single script.\n",
    "\n",
    "**Note:** You would modify the script below to implement your own inferencing logic.\n",
    "\n",
    "Additional information on model loading and model serving for Scikit Learn on SageMaker can be found [here.](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#deploy-a-scikit-learn-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-gross",
   "metadata": {
    "papermill": {
     "duration": 0.02701,
     "end_time": "2021-05-25T01:24:41.302384",
     "exception": false,
     "start_time": "2021-05-25T01:24:41.275374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile $SOURCE_DIR/$INFERENCE_FILE\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    print(\"loading model.joblib from: {}\".format(model_dir))\n",
    "    loaded_model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-distributor",
   "metadata": {
    "papermill": {
     "duration": 0.021138,
     "end_time": "2021-05-25T01:24:41.440775",
     "exception": false,
     "start_time": "2021-05-25T01:24:41.419637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Kick off a model training job for each housing location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-multiple",
   "metadata": {
    "papermill": {
     "duration": 0.027653,
     "end_time": "2021-05-25T01:24:41.489593",
     "exception": false,
     "start_time": "2021-05-25T01:24:41.461940",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_data_locally(location, train, val, test, columns):\n",
    "    _header = \",\".join(columns)\n",
    "\n",
    "    os.makedirs(f\"data/{location}/train\")\n",
    "    np.savetxt(\n",
    "        f\"data/{location}/train/{location}_train.csv\",\n",
    "        train,\n",
    "        delimiter=\",\",\n",
    "        fmt=\"%.2f\",\n",
    "        header=_header,\n",
    "    )\n",
    "\n",
    "    os.makedirs(f\"data/{location}/val\")\n",
    "    np.savetxt(\n",
    "        f\"data/{location}/val/{location}_val.csv\", val, delimiter=\",\", fmt=\"%.2f\", header=_header\n",
    "    )\n",
    "\n",
    "    os.makedirs(f\"data/{location}/test\")\n",
    "    np.savetxt(\n",
    "        f\"data/{location}/test/{location}_test.csv\", test, delimiter=\",\", fmt=\"%.2f\", header=_header\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-thesis",
   "metadata": {
    "papermill": {
     "duration": 11.496365,
     "end_time": "2021-05-25T01:24:53.007155",
     "exception": false,
     "start_time": "2021-05-25T01:24:41.510790",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "estimators = []\n",
    "\n",
    "_houses = gen_houses(NUM_HOUSES)\n",
    "_train, _val, _test = split_data(_houses)\n",
    "save_data_locally(LOCATION, _train, _val, _test, _houses.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-survey",
   "metadata": {
    "papermill": {
     "duration": 0.02104,
     "end_time": "2021-05-25T01:24:41.344552",
     "exception": false,
     "start_time": "2021-05-25T01:24:41.323512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Launch a single training job for a given housing location\n",
    "There is nothing specific to multi-model endpoints in terms of the models it will host. They are trained in the same way as all other SageMaker models. Here we are using the Scikit Learn estimator and not waiting for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-processing",
   "metadata": {
    "papermill": {
     "duration": 0.03286,
     "end_time": "2021-05-25T01:24:41.398536",
     "exception": false,
     "start_time": "2021-05-25T01:24:41.365676",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "\n",
    "# clear out old versions of the data\n",
    "s3_bucket = s3.Bucket(BUCKET)\n",
    "full_input_prefix = f\"{DATA_PREFIX}/model_prep/{LOCATION}\"\n",
    "s3_bucket.objects.filter(Prefix=full_input_prefix + \"/\").delete()\n",
    "\n",
    "# upload the entire set of data for all three channels\n",
    "local_folder = f\"data/{LOCATION}\"\n",
    "inputs = sagemaker_session.upload_data(path=local_folder, key_prefix=full_input_prefix)\n",
    "\n",
    "print(f\"Training data uploaded: {inputs}\")\n",
    "\n",
    "_job = \"skl-{}\".format(LOCATION.replace(\"_\", \"-\"))\n",
    "full_output_prefix = f\"{DATA_PREFIX}/model_artifacts/{LOCATION}\"\n",
    "s3_output_path = f\"s3://{BUCKET}/{full_output_prefix}\"\n",
    "\n",
    "code_location = f\"s3://{BUCKET}/{full_input_prefix}/code\"\n",
    "\n",
    "# Add code_location argument in order to ensure that code_artifacts are stored in the same place.\n",
    "estimator = SKLearn(\n",
    "    entry_point=TRAINING_FILE,  # script to use for training job\n",
    "    role=role,\n",
    "    source_dir=SOURCE_DIR,  # Location of scripts\n",
    "    instance_count=1,\n",
    "    instance_type=TRAIN_INSTANCE_TYPE,\n",
    "    framework_version=\"1.2-1\",  # 1.2-1 is the latest version\n",
    "    output_path=s3_output_path,  # Where to store model artifacts\n",
    "    base_job_name=_job,\n",
    "    code_location=code_location,  # This is where the .tar.gz of the source_dir will be stored\n",
    "    metric_definitions=[{\"Name\": \"median-AE\", \"Regex\": \"AE-at-50th-percentile: ([0-9.]+).*$\"}],\n",
    "    hyperparameters={\"n-estimators\": 100, \"min-samples-leaf\": 3, \"model-name\": LOCATION},\n",
    ")\n",
    "\n",
    "DISTRIBUTION_MODE = \"FullyReplicated\"\n",
    "\n",
    "train_input = TrainingInput(\n",
    "    s3_data=inputs + \"/train\", distribution=DISTRIBUTION_MODE, content_type=\"csv\"\n",
    ")\n",
    "\n",
    "val_input = TrainingInput(\n",
    "    s3_data=inputs + \"/val\", distribution=DISTRIBUTION_MODE, content_type=\"csv\"\n",
    ")\n",
    "\n",
    "remote_inputs = {\"train\": train_input, \"validation\": val_input}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e36e2d-a7c2-47fe-902f-58f4ad1d91d1",
   "metadata": {},
   "source": [
    "## Wait for the job to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ca8d4-7843-4f45-8280-b1ed7fa2bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(remote_inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c8516-6e81-4477-bbff-63edbf6dbf25",
   "metadata": {},
   "source": [
    "## Model data\n",
    "\n",
    "You can check where sagemaker has created the model artifact archive folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866cadf4-58bc-40ad-9baa-f09d42ac523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-conditioning",
   "metadata": {
    "papermill": {
     "duration": 0.023418,
     "end_time": "2021-05-25T01:29:23.902237",
     "exception": false,
     "start_time": "2021-05-25T01:29:23.878819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create the multi-model endpoint with the SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-satellite",
   "metadata": {
    "papermill": {
     "duration": 0.023374,
     "end_time": "2021-05-25T01:29:23.949000",
     "exception": false,
     "start_time": "2021-05-25T01:29:23.925626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create a SageMaker Model from the `estimator`\n",
    "\n",
    "From an `estimator` object, you can create a \"deployable\" model, where you can also specify the `entry_point`, i.e., the python script that can handle the incoming request. To customize your inference on SageMaker hosting, you can build out an `inference.py` file that initializes your model, defines the input and output structure, and produces your inference results. In this file, you can define four functions:\n",
    "\n",
    "- `model_fn` – Initializes your model\n",
    "- `input_fn` – Defines how your data should be input and how to convert to a usable format\n",
    "- `predict_fn` – Takes the input data and receives the prediction\n",
    "- `output_fn` – Converts the prediction into an API call format\n",
    "\n",
    "In out example, we only provide the custom `model_fn` that overwrites the default method to tell SageMaker how to load the model in memory when the worker is initialized. All other functions uses the default methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-serve",
   "metadata": {
    "papermill": {
     "duration": 0.051364,
     "end_time": "2021-05-25T01:29:24.023784",
     "exception": false,
     "start_time": "2021-05-25T01:29:23.972420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inference.py is the entry_point for when we deploy the model\n",
    "# Note how we do NOT specify source_dir again, this information is inherited from the estimator\n",
    "model = estimator.create_model(role=role, entry_point=\"inference.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-daughter",
   "metadata": {
    "papermill": {
     "duration": 0.024051,
     "end_time": "2021-05-25T01:29:24.281257",
     "exception": false,
     "start_time": "2021-05-25T01:29:24.257206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Deploy the Model Endpoint\n",
    "\n",
    "You need to consider the appropriate instance type and number of instances for the projected prediction workload the model you plan to host behind your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-whole",
   "metadata": {
    "papermill": {
     "duration": 451.983062,
     "end_time": "2021-05-25T01:36:56.287962",
     "exception": false,
     "start_time": "2021-05-25T01:29:24.304900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1, instance_type=ENDPOINT_INSTANCE_TYPE, endpoint_name=ENDPOINT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-shelter",
   "metadata": {
    "papermill": {
     "duration": 0.027137,
     "end_time": "2021-05-25T01:36:58.190222",
     "exception": false,
     "start_time": "2021-05-25T01:36:58.163085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get predictions from the endpoint\n",
    "\n",
    "Recall that ```model.deploy()``` returns a [RealTimePredictor](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/predictor.py#L35) that we saved in a variable called ```predictor```.\n",
    "\n",
    "We will use ```predictor``` to submit requests to the endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-exchange",
   "metadata": {
    "papermill": {
     "duration": 0.027017,
     "end_time": "2021-05-25T01:36:58.244097",
     "exception": false,
     "start_time": "2021-05-25T01:36:58.217080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Invoking models on a multi-model endpoint\n",
    "Notice the higher latencies on the first invocation of any given model. This is due to the time it takes SageMaker to download the model to the Endpoint instance and then load the model into the inference container. Subsequent invocations of the same model take advantage of the model already being loaded into the inference container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-arrow",
   "metadata": {
    "papermill": {
     "duration": 2.176149,
     "end_time": "2021-05-25T01:37:00.447411",
     "exception": false,
     "start_time": "2021-05-25T01:36:58.271262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "payload = gen_random_house()[1:]\n",
    "predicted_value = predictor.predict(data=[payload])\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(payload)\n",
    "print(\"${:,.2f}, took {:,d} ms\\n\".format(predicted_value[0], int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-morgan",
   "metadata": {
    "papermill": {
     "duration": 0.02806,
     "end_time": "2021-05-25T01:37:02.014540",
     "exception": false,
     "start_time": "2021-05-25T01:37:01.986480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Using Boto APIs to invoke the endpoint\n",
    "\n",
    "While developing interactively within a Jupyter notebook, since `.deploy()` returns a `RealTimePredictor` it is a more seamless experience to start invoking your endpoint using the SageMaker SDK. You have more fine grained control over the serialization and deserialization protocols to shape your request and response payloads to/from the endpoint. This is the approach we demonstrated above where the `RealTimePredictor` was stored in the variable `predictor`.\n",
    "\n",
    "This is great for iterative experimentation within a notebook. Furthermore, should you have an application that has access to the SageMaker SDK, you can always import `RealTimePredictor` and attach it to an existing endpoint - this allows you to stick to using the high level SDK if preferable.\n",
    "\n",
    "Additional documentation on `RealTimePredictor` can be found [here.](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html?highlight=RealTimePredictor#sagemaker.predictor.RealTimePredictor)\n",
    "\n",
    "The lower level Boto3 SDK may be preferable if you are attempting to invoke the endpoint as a part of a broader architecture.\n",
    "\n",
    "Imagine an API gateway frontend that uses a Lambda Proxy in order to transform request payloads before hitting a SageMaker Endpoint - in this example, Lambda does not have access to the SageMaker Python SDK, and as such, Boto3 can still allow you to interact with your endpoint and serve inference requests.\n",
    "\n",
    "Boto3 allows for quick injection of ML intelligence via SageMaker Endpoints into existing applications with minimal/no refactoring to existing code.\n",
    "\n",
    "Boto3 will submit your requests as a binary payload, while still allowing you to supply your desired `Content-Type` and `Accept` headers with serialization being handled by the inference container in the SageMaker Endpoint.\n",
    "\n",
    "Additional documentation on `.invoke_endpoint()` can be found [here.](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-colony",
   "metadata": {
    "papermill": {
     "duration": 0.038412,
     "end_time": "2021-05-25T01:37:02.081270",
     "exception": false,
     "start_time": "2021-05-25T01:37:02.042858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "\n",
    "\n",
    "def predict_one_house_value(features):\n",
    "    print(f\"predict price of this house: {features}\")\n",
    "\n",
    "    float_features = [float(i) for i in features]\n",
    "\n",
    "    body = json.dumps([float_features])\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=ENDPOINT_NAME, ContentType=\"application/json\", Body=body\n",
    "    )\n",
    "\n",
    "    predicted_value = json.loads(response[\"Body\"].read())[0]\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    print(\"${:,.2f}, took {:,d} ms\\n\".format(predicted_value, int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-comparative",
   "metadata": {
    "papermill": {
     "duration": 0.197522,
     "end_time": "2021-05-25T01:37:02.307002",
     "exception": false,
     "start_time": "2021-05-25T01:37:02.109480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_one_house_value(gen_random_house()[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-camping",
   "metadata": {
    "papermill": {
     "duration": 0.02832,
     "end_time": "2021-05-25T01:37:02.363598",
     "exception": false,
     "start_time": "2021-05-25T01:37:02.335278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clean up\n",
    "Here, to be sure we are not billed for endpoints we are no longer using, we clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-sleeping",
   "metadata": {
    "papermill": {
     "duration": 0.225499,
     "end_time": "2021-05-25T01:37:02.617474",
     "exception": false,
     "start_time": "2021-05-25T01:37:02.391975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50698e1-61a1-48b3-8d4a-f44757211fa9",
   "metadata": {},
   "source": [
    "Remove folders created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a264adf-27b2-41c9-8c14-3de730f52d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data source_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 744.990703,
   "end_time": "2021-05-25T01:37:03.151731",
   "environment_variables": {},
   "exception": null,
   "input_path": "sklearn_multi_model_endpoint_home_value.ipynb",
   "output_path": "/opt/ml/processing/output/sklearn_multi_model_endpoint_home_value-2021-05-25-01-20-50.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25"
   },
   "start_time": "2021-05-25T01:24:38.161028",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
